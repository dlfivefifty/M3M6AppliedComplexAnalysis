__M3M6: Methods of Mathematical Physics__

Dr. Sheehan Olver

s.olver@imperial.ac.uk

# Lecture 9: Computing matrix functions via the trapezium rule

Here we look at computing matrix functions via discretising Cauchy's integral formula with the Trapezium rule. That is,
we saw that we can define matrix functions via
$$
f(A) = {1 \over 2 \pi \I} \oint_\gamma f(\zeta) (\zeta I - A)^{-1} \D \zeta
$$
where $\gamma$ is a contour surrounding the spectrum such that $f$ is analytic in the interior, often a circle or an ellipse. If we parameterise the curve from the circle
$\gamma : [0,2 \pi) \rightarrow \CC$ then we can apply Trapezium rule:
$$
f(A) = {1 \over 2 \pi \I} \int_0^{2\pi}  f(\gamma(\theta)) (\gamma(\theta) I - A)^{-1}\gamma'(\theta)  \D \theta \approx 
{1 \over \I N}  \sum_{j=0}^{N-1} f(\gamma(\theta_j)) \gamma'(\theta_j) (\gamma(\theta_j) I - A)^{-1}.
$$
Thus matrix functions are reduced to a sum of inverses. This is useful if applying an inverse is fast, for example, we have
$$
f(A) \vc v \approx {1 \over \I N}  \sum_{j=0}^{N-1} f(\gamma(\theta_j)) \gamma'(\theta_j) (\gamma(\theta_j) I - A)^{-1} \vc v
$$
and if $A$ is sparse then each inverse is fast. 



## Example: Matrix exponentials

Let $A \in {\mathbb C}^{d \times d}$,   ${\mathbf u}_0 \in {\mathbb C}^d$ and consider the constant coefficient linear ODE
$$
    {\mathbf u}'(t) = A {\mathbf u}(t)\qquad\hbox{and}\qquad {\mathbf u}(0) = {\mathbf u}_0(0)
$$
    
The solution to this is given by the _matrix exponential_
$$
    {\mathbf u}(t) = \exp(A t) {\mathbf u}_0
$$
where the matrix exponential is defined by it's Taylor series:
$$
    \exp(A) = \sum_{k=0}^\infty {A^k \over k!}
$$
This has stability problems, so a more convenient form is as follows:


_Demonstration_ we use this formula alongside the complex trapezium rule to calculate matrix exponentials. 
Begin by creating a random symmetric matrix (which only has real eigenvalues):
```julia
using LinearAlgebra, Plots, ComplexPhasePortrait, ApproxFun

A = randn(5,5)
A = A + A'
λ = eigvals(A)
```
We can now by hand create a circle that surrounds all the eigenvalues:
```julia
periodic_rule(N) = 2π/N*(0:(N-1)), 2π/N*ones(N)
function circle_rule(n, r) 
    θ = periodic_rule(n)[1]
    r*exp.(im*θ), 2π*im*r/n*exp.(im*θ)
end
z,w = circle_rule(100,maximum(abs.(λ))+1)

plot(z)
scatter!(λ,zeros(5); label = "eigenvalues of A")
```
Here we wrap this up into a function `circle_exp` that calculates the matrix exponential:
```julia
function circle_exp(A, n, z₀, r)
    z,w = circle_rule(n,r)
    z .+= z₀

    ret = zero(A)
    for j=1:n
        ret += w[j]*exp(z[j])*inv(z[j]*I - A)
    end

    ret/(2π*im)
end
   
circle_exp(A, 100, 0, 8.0) -exp(A) |>norm
```
In this case, it is beneficial to use an ellipse:
```julia
function ellipse_rule(n, a, b) 
    θ = periodic_rule(n)[1]
    a*cos.(θ) + b*im*sin.(θ), 2π/n*(-a*sin.(θ) + im*b*cos.(θ))
end
function ellipse_exp(A, n, z₀, a, b)
    z,w = ellipse_rule(n,a,b)
    z .+= z₀

    ret = zero(A)
    for j=1:n
        ret += w[j]*exp(z[j])*inv(z[j]*I - A)
    end
    ret/(2π*im)
end

ellipse_exp(A, 50, 0, 8.0, 5.0) -exp(A) |>norm
```
For matrices with large negative eigenvalues (For example, discretisations of the Laplacian), complex quadrature can lead to much better accuracy than Taylor series:
```julia
function taylor_exp(A,n)
    ret = Matrix(I, size(A))
    for k=1:n
        ret += A^k/factorial(1.0k)
    end
    ret
end

B = A - 20I

taylor_exp(B, 200) -exp(B) |>norm
```
We can use an ellpise to surround the spectrum:
```julia
scatter(complex.(eigvals(B)))
plot!(ellipse_rule(50,8,5)[1] .- 20)

norm(ellipse_exp(B, 50, -20.0, 8.0, 5.0) - exp(B))
```

## Finite differences and (fractional) heat equation
As a first application we consider approximation of the solution of the heat equation $u(t,x)$, for $\Delta u := u_{xx}$ we have:
$$
u_t = \Delta u
$$
on $[0,1]$ with Dirichlet conditions $u(t,0) = u(t,1) = 0$ and initial condition $u(0,x) = u_0(x)$. 
We approximate the solution by its values at a grid, that is, we have time dependent vector $\vc u(t)$ 
which we hope satisfies the property that at any time $t$
$$
\vc u(t) \approx \Vectt[u({t,x_1}), \dots, u({t,x_N})]
$$
where $x_j = j/(N+1)$ is an evenly spaced grid. Note we use that $u(t,x_0) = u(t,0) = 0$ and $u(t,x_{N+1}) = u(t,1) = 0$.

Recall from Taylor series that for a smooth enough function $f(x)$ we have 
$$
\begin{align*}
{f(x+h) - 2 f(x) + f(x-h) \over h^2} &\approx {f(x) + f'(x) h + f''(x) h^2/2 - 2 f(x) + f(x) - f'(x) h + f''(x) h^2/2  \over h^2}\\
&= f''(x)
\end{align*}
$$
For $h = 1/(N+1)$ this therefore gives
$$
\begin{align*}
u_xx(t,x_1) & \approx {u(t,x_2) - 2 u(t,x_1) + u(t,x_0) \over h^2} = {u(t,x_2) - 2 u(t,x_1) \over h^2}  \\
u_xx(t,x_N) & \approx {u(t,x_{N+1}) - 2 u(t,x_N) + u(t,x_{N-1}) \over h^2} = { - 2 u(t,x_N) + u(t,x_{N-1}) \over h^2}  \\
u_xx(t,x_j) & \approx {u(t,x_{j+1}) - 2 u(t,x_j) + u(t,x_{j-1}) \over h^2}
\end{align*}
$$
Or in vector form we have
$$
\Vectt[{u_xx(t,x_1)}, \dots, {u_xx(t,x_N)}] \approx \Delta \Vectt[{u(t,x_1)}, \dots, {u(t,x_N)}]
$$
where 
$$
\Delta_N = {1 \over h^2} \sopmatrix{-2 & 1 \\ 1 & -2 & 1 \\ & \ddots & \ddots & \ddots \\ && 1 & -2 & 1 \\ &&&1 &-2 }
$$
In short: a natural approximation to the heat equation is
$$
\vc u'(t) = \Delta_N \vc u(t), \qquad u(0) = \vc u_0
$$
where $\vc u_0 = \vectt[u_0(x_1), \dots, u_0(x_N)]$. Thus to evaluate the solution at time $t$ we have
$$
\vc u(t) = \E^{\Delta_N t} \vc u_0 \approx {1 \over 2\pi \I} \oint_\gamma \E^{\zeta t} (\zeta I - \Delta_N)^{-1} \vc u_0 \D \zeta.
$$
We need to surround the spectrum of $\Delta_N$. In this case it suffices to use Gershgorin's theorem to determine:
$$
\sigma(\Delta_N) \subset [-4/h^2,0] = [-4 (N+1)^2,0]
$$

Now for something more complicated, the fractional heat equation:
$$
u_t = - (-\Delta)^\alpha u
$$
where $\alpha$ is say $1/2$. These come up in applications involving "memory", for example, in medical imaging where 
tissue changes over time. They also come up in Levy flights, that is, stochastic differential equations with heavy-tailed 
distributions. The precise definition is beyond the scope of this course, but in terms of numerical approximation it is
natural to discretise as
$$
\vc u'(t) = - (-\Delta_N)^\alpha \vc u
$$
Or in other words,
$$
\vc u(t) = \E^{-(-\Delta_N)^\alpha t} \vc u_0 \approx {1 \over 2\pi \I} \oint_\gamma \E^{-\zeta^\alpha t} (\zeta I + \Delta_N)^{-1}  \vc u_0 \D \zeta.
$$
Gershgorin's theorem does not help us: it states 
$$
\sigma(-\Delta_N) \subset [0,4/h^2]
$$
but $\E^{- \zeta^\alpha t }$ has a branch cut right at 0. Fortunately in this case we have more understanding of the spectrum of $\Delta_N$. In 
particular, for 
$$
J = \sopmatrix{0 & 1 \\ 1 & 0 & 1 \\ & \ddots & \ddots & \ddots \\ && 1 & 0 & 1 \\ &&& 1 & 0}
$$
we have from trigonometric relationships 
$$
2\cos \theta \sin k \theta = \sin (k+1) \theta + \sin(k-1) \theta
$$
that
$$
\begin{align*}
J \Vectt[\sin \theta, \sin 2 \theta , \dots, \sin N \theta] = \Vectt[\sin 2 \theta, \sin 3 \theta + \sin \theta ,\dots, \sin(N-2) \theta + \sin N \theta, \sin (N-1) \theta ]\\
&= 2 \cos \theta\Vectt[\sin \theta, \sin 2 \theta , \dots, \sin (N-1) \theta,\sin N \theta] - \Vectt[0,0,\dots,0,\sin (N+1) \theta]
\end{align*}
$$
The second term is zero when $\theta = \theta_j = j \pi / (N+1)$, thus we know the eigenvalues of $\Delta$ are $\lambda_j = 2 \cos \theta_j$
for $j = 1,\ldots,N$. ($j = 0$ is not valid as then the "eigenvector" would be zero.)
We need to bound the spectrum away from $0$ and $2$. First note from telescoping properties of cosine Taylor series we have
$$
\cos x = 1 - x^2/2  + x^4/24 - \cdots \leq 1 - x^2/2 + x^4/24
$$
thus our bound above is
$$
2 \cos \theta_1 = 2 \cos {\pi  \over N+1} \leq 2 - \pr({\pi \over N+1})^2 + \pr({\pi \over N+1})^4/12
$$
The same holds for the bound below:
$$
2 \cos \theta_N \geq -2 + \pr({\pi \over N+1})^2 - {\pi^4 \over 12 (N+1)^2}
$$
Thus we have
$$
\sigma(\Delta_N) = \sigma(J - 2I)/h^2 \subset \br[-4 (N+1)^2 + \pi^2 - {\pi^4 \over 12 (N+1)^2},-\pi^2 + {\pi^4 \over 12 (N+1)^2}]
$$
Thus it suffices to take an ellipse contour passing through say $\pi^2/2$ and $-4 (N+1)^2$.



__Demonstration__
We first demonstrate using the built-in matrix exponential. We can construct $\Delta$ as follows:
```julia
N = 20
h = 1/N
Δ = SymTridiagonal(fill(-2,N), fill(1,N-1))/h^2
```
The solution with an initial condition $u_0(x) = x (1-x) \E^x$ is then:
```julia
u0 = x -> x * (1-x) * exp(x)

xx = range(0,1; length=N+2)
A = Matrix(Δ) # Need to convert to dense matrix to diagonalise
p = plot()
for t in 0:0.1:0.5
    plot!(xx, [0; exp(A*t)*u0.(xx[2:end-1]); 0]; label="t = $t")
end
p
```
The above calculation is expensive: it does not take into account the sparsity of `Δ`. To do 
better we use Cauchy's integral formula with trapezium rule. First design an ellipse that surrounds the spectrum:
```julia
m = 20_000
z,w = ellipse_rule(m,2/h^2,1)
z .-= 2/h^2 # shift to negative
plot(z; label="contour")
scatter!(eigvals(Δ), zeros(N); markersize=0.5, label="eigenvalues")
```
We thus can evaluate the matrix exponential at time $t$ using the quadrature rule:
```julia
u0_v = u0.(xx[2:end-1])
ret = zero(complex(u0_v))
t = 0.5
for j=1:length(z)
    global ret, w, z
    ret += w[j]*exp(t*z[j]) * ((z[j]*I - Δ) \ u0_v)
end
norm(ret/(2π*im) - exp(A*t)*u0_v) # matches to high accuracy
```

For fractional heat equation, we have slower diffusion:

```julia
A = -sqrt(-Matrix(Δ))
p = plot()
for t in 0:0.1:0.5
    plot!(xx, [0; exp(A*t)*u0.(xx[2:end-1]); 0]; label="t = $t")
end
p
```

We need to be more careful and navigate the branch cut of $\E^{-\sqrt{z}}$. That is, we need the following contour
(note we evaluate on $-\Delta$):
```julia
m = 20_000
z,w = ellipse_rule(m,2/h^2,1)
z .+= 2/h^2 + π^2/2 # shift to positive
plot(z; label="contour")
scatter!(eigvals(-Δ), zeros(N); markersize=0.5, label="eigenvalues")
plot!([-2000,0],[0,0]; label="branch cut")

``` 
```julia
u0_v = u0.(xx[2:end-1])
ret = zero(complex(u0_v))
t = 0.5
for j=1:length(z)
    global ret, w, z
    ret += w[j]*exp(-t*sqrt(z[j])) * ((z[j]*I + Δ) \ u0_v)
end
norm(ret/(2π*im) - exp(A*t)*u0_v) # matches to high accuracy
```


In both cases this is the start, not the end, of a very interesting computational problem. 
As `N` becomes large there it is delicate to get the shape of the contour and the number of quadrature
points right to get a desired accuracy. Higham, Functions of Matrices: Theory and Computation, SIAM, 2008 
provides a starting point with a number of more recent developments.